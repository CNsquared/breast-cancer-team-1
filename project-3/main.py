import joblib
import pandas as pd
from src.utils.preprocess import MutPreprocessor
from src.utils.metadata_utils import load_metadata, merge_with_components
from src.models.nmf_runner import NMFDecomposer
from src.utils.enrichment_tests import test_association
from src.models.signature_comparator import load_sigprofiler_results, cosine_similarity
from src.models.clustering import consensus_signatures

# paths
MUTATIONS_PATH = "data/raw/TCGA.BRCA.mutations.txt"
METADATA_PATH = "data/raw/TCGA.BRCA.metadata.txt"
SIGPROFILER_PATH = "data/raw/sigprofiler_results.txt"
ASSOCIATION_COLUMNS=['sex','age','cancer_subtype']

# NMF parameters - Identify best parameters for our dataset
# PAPER_NMF_PARAMS = {
#     'n_components': 5,
#     'resample_method': 'poisson',
#     'objective_function': 'frobenius',
#     'initialization_method': 'random',
#     'normalization_method': 'GMM',
#     'max_iter': 1000000,
#     'num_factorizations': 100,
#     'random_state': 42,
#     'tolerance': 1e-15
# }

NMF_PARAMS = {
    'n_components': 25,  # number of signatures to extract
    'resample_method': 'poisson', # resampling method used on normalized X'
    'objective_function': 'frobenius', # objective function to minimize for nmf
    'initialization_method': 'random', # initialization method for S and A
    'normalization_method': 'GMM', # normalization applied to X before resampling
    'max_iter': 10000, # maximum number of mulitiplicative updates in nmf algo
    'num_factorizations': 100, # number of times to run nmf (with independent resampling/normalization)
    'random_state': 42,
    'tolerance': 1e-6 # tolerance for convergence of nmf
}

def main():

    #Preprocess MAF and generate mutation matrix
    print("Preprocessing mutation data done in bash and mutation matrix generated by SigProfilerMatrixGenerator")
    pre = MutPreprocessor(MUTATIONS_PATH)
    # matrix = pre.get_mutation_matrix()
    # X, sample_ids, feature_names = matrix['X'], matrix['sample_ids'], matrix['feature_names']
    
    df = pd.read_csv("data/processed/BRCA.SBS96.all", sep="\t")

    # 2. Drop the "MutationType" column
    feature_names = df["MutationType"].tolist()
    df_matrix = df.drop(columns=["MutationType"])
    sample_ids = list(df_matrix.columns)
    
    X = df_matrix.to_numpy()
    print(f"Loaded {len(sample_ids)} samples and {len(feature_names)} features.")
    print(f"Mutation matrix shape: {X.shape}")
    print(X)
    df_mut = pre.get_processed_df()

    # save processed data
    df.to_csv("data/processed/TCGA.BRCA.mutations.qc1.csv", index=False)  

    # -----------------------------------------------------------
    # run NMF for some value of k, num_factorizations times

    print("Running NMF decomposition...")
    nmf_model = NMFDecomposer(**NMF_PARAMS)
    S_all, A_all, err_all = nmf_model.run(X)

    # save NMF results
    joblib.dump({'S_all': S_all, 'A_all': A_all, 'err_all': err_all}, 'data/processed/nmf_replicates.joblib')

    # -----------------------------------------------------------
    # cluster NMF results to build consensus S and A

    print("Partition clustering NMF results...")
    # TODO: alex's function/class
    centriods = consensus_signatures(S_all, k = 5, stability_threshold=0.8, run_threshold=0.8)

    # -----------------------------------------------------------
    # annotate metadata and see if we can find associations with signatures

    # load metadata
    print("Loading and merging metadata...")
    metadata = load_metadata(METADATA_PATH)
    S_annotated = merge_with_components(centriods, sample_ids, metadata)

    # save cleaned metadata and annotated W
    metadata.to_csv("data/processed/TCGA.BRCA.metadata.qc1.csv", index=False)
    joblib.dump(S_annotated, "data/processed/S_annotated.joblib")

    # statistical association tests
    print("Testing associations with metadata...")
    results = test_association(S_annotated, test_cols=ASSOCIATION_COLUMNS)

    # save results
    results.to_csv("reports/enrichment_results.csv", index=False)
    
    # -----------------------------------------------------------
    # compare with sigprofiler results

    print("Loading and merging sigprofiler results...")
    sigprofiler = load_sigprofiler_results(SIGPROFILER_PATH)
    cos_sim = cosine_similarity(W, sigprofiler)

    # save cosine similarity results
    cos_sim_df = pd.DataFrame(cos_sim, columns=["sigprofiler_signature_1", "sigprofiler_signature_2", "cosine_similarity"])
    cos_sim_df.to_csv("reports/cosine_similarity_results.csv", index=False)
    print("All steps completed successfully.")    

if __name__ == "__main__":
    main()
