import joblib
import pandas as pd
from src.utils.preprocess import MutPreprocessor
from src.utils.metadata_utils import load_metadata, merge_with_components
from src.models.nmf_runner_parallel import NMFDecomposer
from src.utils.enrichment_tests import test_association
from src.models.signature_comparator import load_sigprofiler_results, cosine_similarity
from src.models.clustering import consensus_signatures
import os
#import torch
#from src.models.nmf_runner_gpu import NMFDecomposer as NMFDecomposer_GPU

RERUN_NMF=False
VERBOSE=True
GPU = False #torch.cuda.is_available()

# paths
MUTATIONS_PATH = "data/raw/TCGA.BRCA.mutations.txt"
METADATA_PATH = "data/raw/TCGA.BRCA.metadata.txt"
SIGPROFILER_PATH = "data/raw/sigprofiler_results.txt"
ASSOCIATION_COLUMNS=['sex','age','cancer_subtype']

# NMF parameters - Identify best parameters for our dataset
# PAPER_NMF_PARAMS = {
#     'n_components': 5,
#     'resample_method': 'poisson',
#     'objective_function': 'frobenius',
#     'initialization_method': 'random',
#     'normalization_method': 'GMM',
#     'max_iter': 1000000,
#     'num_factorizations': 100,
#     'random_state': 42,
#     'tolerance': 1e-15
# }

NMF_PARAMS = {
    'n_components': 2,  # number of signatures to extract
    'resample_method': 'poisson', # resampling method used on normalized X'
    'objective_function': 'frobenius', # objective function to minimize for nmf
    'initialization_method': 'random', # initialization method for S and A
    'normalization_method': 'GMM', # normalization applied to X before resampling
    'max_iter': 10000, # maximum number of mulitiplicative updates in nmf algo
    'num_factorizations': 100, # number of times to run nmf (with independent resampling/normalization)
    'random_state': 42,
    'tolerance': 1e-6 # tolerance for convergence of nmf
}

def main():

    #Preprocess MAF and generate mutation matrix
    print("Preprocessing mutation data done in bash and mutation matrix generated by SigProfilerMatrixGenerator")
    #pre = MutPreprocessor(MUTATIONS_PATH)
    # matrix = pre.get_mutation_matrix()
    # X, sample_ids, feature_names = matrix['X'], matrix['sample_ids'], matrix['feature_names']
    
    df = pd.read_csv("data/processed/BRCA.SBS96.all", sep="\t")

    # 2. Drop the "MutationType" column
    feature_names = df["MutationType"].tolist()
    df_matrix = df.drop(columns=["MutationType"])
    sample_ids = list(df_matrix.columns)
    
    X = df_matrix.to_numpy()
    print(f"Loaded {len(sample_ids)} samples and {len(feature_names)} features.")
    print(f"Mutation matrix shape: {X.shape}")
    print(X)
    #df_mut = pre.get_processed_df()

    # save processed data
    df.to_csv("data/processed/TCGA.BRCA.mutations.qc1.csv", index=False)  


    # -----------------------------------------------------------
    # run NMF for some value of k, num_factorizations times

    nmf_file = "data/processed/nmf_replicates.joblib"
    if os.path.exists(nmf_file) and not RERUN_NMF:
        print("RERUN_NMF is set to False and nmf files exist. Loading existing NMF results...")
        data = joblib.load(nmf_file)
        S_all, A_all, err_all = data["S_all"], data["A_all"], data["err_all"]
    else:
        if GPU :
            pass
            '''
            print("Running NMF decomposition with GPU...")
            #nmf_model = NMFDecomposer_GPU(**NMF_PARAMS, verbose=VERBOSE)
            S_all, A_all, err_all, _ = nmf_model.run(X)
            joblib.dump({"S_all": S_all, "A_all": A_all, "err_all": err_all}, nmf_file)
            '''
        else:
            print("Running NMF decomposition...")
            nmf_model = NMFDecomposer(**NMF_PARAMS, verbose=VERBOSE)
            S_all, A_all, err_all, _ = nmf_model.run(X)
            joblib.dump({"S_all": S_all, "A_all": A_all, "err_all": err_all}, nmf_file)

    # -----------------------------------------------------------
    # cluster NMF results to build consensus S and A

    print("Partition clustering NMF results...")
    centroids_s, all_centroids_s, sil_score_s = consensus_signatures(X, S_all, k = 2, average_threshold=0.8, minimum_threshold=0.2)
    #centroids_s, sil_score_s, recon_err_s = consensus_signatures(X, S_all, k = 25, average_threshold=-1, minimum_threshold=-1)

    import numpy as np
    import time
    
    
    #Calculate the signature weights using NNLS
    from scipy.optimize import nnls
    centroids_s = np.array(centroids_s)
    all_centroids_s = np.array(all_centroids_s)
    #print(f"Centroids shape: {centroids_s.shape}")
    
    # centroids_s: (k, features), X: (features, num_samples)
    k, num_samples = all_centroids_s.shape[0], X.shape[1]
    signature_weights_nnls = np.zeros((k, num_samples))

    for j in range(num_samples):
        # solve centroids_s.T @ w = X[:, j]  with w â‰¥ 0
        signature_weights_nnls[:, j], _ = nnls(all_centroids_s.T, X[:, j])
    print(f"Signature weights shape from nnls: {signature_weights_nnls.shape}")
    

    print("-" * 50)
    
    


    # -----------------------------------------------------------
    # annotate metadata and see if we can find associations with signatures

    # load metadata
    print("Loading and merging metadata...")
    metadata = load_metadata(METADATA_PATH)
    S_annotated = merge_with_components(centroids_s, sample_ids, metadata)

    # save cleaned metadata and annotated W
    metadata.to_csv("data/processed/TCGA.BRCA.metadata.qc1.csv", index=False)
    joblib.dump(S_annotated, "data/processed/S_annotated.joblib")

    # statistical association tests
    print("Testing associations with metadata...")
    results = test_association(S_annotated, test_cols=ASSOCIATION_COLUMNS)

    # save results
    results.to_csv("reports/enrichment_results.csv", index=False)
    
    # -----------------------------------------------------------
    # compare with sigprofiler results

    print("Loading and merging sigprofiler results...")
    sigprofiler = load_sigprofiler_results(SIGPROFILER_PATH)
    cos_sim = cosine_similarity(W, sigprofiler)

    # save cosine similarity results
    cos_sim_df = pd.DataFrame(cos_sim, columns=["sigprofiler_signature_1", "sigprofiler_signature_2", "cosine_similarity"])
    cos_sim_df.to_csv("reports/cosine_similarity_results.csv", index=False)
    print("All steps completed successfully.")    

if __name__ == "__main__":
    main()
